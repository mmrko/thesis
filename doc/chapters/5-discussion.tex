% !TEX root = ../thesis.tex

% Tutkimustuloksien merkitystä on aina syytä arvioida ja tarkastella
% kriittisesti. Tässä osassa on syytä myäs arvioida tutkimustulosten luotettavuutta.

% At this point, you will have some insightful thoughts on your implementation
% and you may have ideas on what could be done in the future. This chapter
% is a good place to discuss your thesis as a whole and to show your professor
% that you have really understood some non-trivial aspects of the methods you
% used. . .

\documentclass[thesis.tex]{subfiles}

\begin{document}

\chapter{Discussion}
\label{chapter:discussion}

This chapter discusses the design decisions taken during the implementation of the application and the findings made based on the the raw result data and the parts of it summarized in \ref{chapter:results}. Last, challenges faced during the project and an outline for future research work are presented. Motivation behind some of the design choices mentioned earlier in the writing are not reiterated here, and the interested reader is instead advised to refer to Chapter \ref{chapter:design-implementation} and the related appendices \ref{appendix:camera-module} and \ref{appendix:capture-presets}.

\section{Design Choices}

One of the challenges early on in the project was the capture of a taggant's photoexcitation. A 3D printed version of the camera module had been developed prior the project for demonstration purposes. It provided good isolation from ambient light and even a built-in slot for different diffraction gratings. It however did not include an integrated light source, and taggants were instead excited manually by toggling a UV flashlight (Nightsearcher UV 365). A programmatic control over the light source was required for product authentication purposes. And, as integration of a smartphone and a light source with the existing 3D printed mold proved difficult, a cardboard prototype was implemented, as presented in \ref{chapter:camera-module}.

The smartphone torch light in both the Samsung S4 and Lumia 1020 did not emit the right kind (wavelength) of light to be able to excite the taggants, and thus, an external light source was required. An external light source would also help in normalizing differences between smartphones and the various kinds of wavelengths their torch lights emit. Programmatic control of the light source required a connection to be established between the smartphone and the light source. Cross-modal communication by means of light signals proved to be a straightforward solution. Connecting the two devices via WiFi, Bluetooth or cable would have added unnecessary complexity in terms of the use case.

As one of the goals of the project was to be able to compare results across different smartphones (as per \ref{R1}) differences in the capture method needed to be normalized. For this purpose a set capture presets were defined. The preset values for the \emph{interval} duration were configured based on the taggants. As described in \cite{luminova}, the Luminova\textregistered pigments have a fairly long emission duration. For the smartphone to be able to capture any significant change in the emission the interval also needs to be long. The appropriate intervals of 200, 400 and 600ms were eventually derived empirically.

The number of frames (samples) to capture per fingerprint was fixed at five. It was empirically observed that additional frames would not provide much data due to decreasing Signal-to-noise ratio. Furthermore, capturing more frames would have led to worse experience for the user as the capture and processing times would have gotten longer. Capturing three or four frames could have provided similar results, but as emission peaks could still be detected even after 3000ms for some of the taggants, the frame count was not adjusted. Moreover, it would be trivial to retroactively compute results against four or less frames if necessary.


- Daylight WB because Yongnuo YN565EX (5600K temp) \& least compensation

could not set iso on Android, thus AUTO

- decay time Android 200-250ms because of led flash vs. WP's Xenon flash decay

- Focus distance and torch were adjusted as per the platform: S4 does not support setting focus to a fixed minimum distance, and thus, macro focusing was used to guarantee the camera would be able to focus on the taggant. However this

- The preview feed was used since the Android and Windows Phone versions (Kitkat and 8.1, respectively) lacked the proper support for burst mode (image capture at a predefined interval). Furthermore, frames could not be captured individually as Thus, there was no way to schedule the capture to take place at a specific interval.

- values of analysis and matching param values?

- normalized light source required, cant use builtin doesnt excite

- Eventual consistency (CouchDB) (http://guide.couchdb.org/draft/consistency.html, http://pouchdb.com/guides/replication.html)
- Schema flexibility (NoSQL) SYNC!


\section{Findings}
- which taggants perform the best, and why?
- SP <-> O (similar)

- sample a and b not really siblings per se: an\_400\_r/13SVa vs. an\_400\_r/13SVb (taggant preparation)
- Matches against xxx\_r vs the b sample points to the fact that the taggant creation process plays a big role...

- good match: an\_600/24SPa, an\_600\_r/24VPb, wp\_600/24SPa, wp\_600/24SPb

- wp\_600/13VPb: matches everything with fingerprint method, only a few with histogram (=> anomaly)
It was observed from the raw data that wp\_600/13VPb matched surprisingly poorly with rest of the corpus. This was due to (persistance threshold, residue afterglow?):

- example of exposure lock behaviour on Android: [some pic]

% /Users/Make/Documents/dippa/app/luminotrace-native/corpus/data/wp_600/13VPb/2015-05-06-004852
vs.
% /Users/Make/Documents/dippa/app/luminotrace-native/corpus/data/wp_400/13VPb/2015-05-05-184418

- Margin increase => minimal impact for both approaches
- Fingerprint method: more misses (histogram method more stable)
- peak method performs poorly against the histogram approach for single coloured luminophors
- \ref{equation:similarity-metric-no-peaks} - shortcoming of the fingerprint method?

Artefacts!

\section{Challenges and Future Work}
\begin{comment}
Color calibration

There are mainly two modules responsible for the color-rendering accuracy of a digital camera: the former is the illuminant estimation and correction module, and the latter is the color matrix transformation aimed to adapt the color response of the sensor to a standard color space. These two modules together form what may be called the color correction pipeline.

RGB is a device-dependent color model: different devices detect or reproduce a given RGB value differently, since the color elements (such as phosphors or dyes) and their response to the individual R, G, and B levels vary from manufacturer to manufacturer, or even in the same device over time. Thus an RGB value does not define the same color across devices without some kind of color management.
\url{http://www.cis.rit.edu/~jxj1770/publications/paperEI_Xerox.pdf}
\url{http://www.cs.unc.edu/techreports/04-012.pdf}

% https://www.cs.unc.edu/~welch/media/pdf/Ilie2005_Calib.pdf
% Unfortunately most cameras—even of the same typedo not exhibit consistent responses. Figure 1 illustrates the
% differences between the responses of 8 cameras to the 24 colors of the GretagMacbeth [5] ColorCheckerTM chart
% imaged under the same illumination conditions and using the same hardware settings. The data shows that color
% values are significantly different from camera to camera. This is due for example to aperture variations, fabrication
% variations, electrical noise, and interpolation artifacts arising from the reconstruction of a full-resolution color image
% from a half-resolution Bayer pattern image

\end{comment}

- thus it be better to add some structural \& spatial similarity semantics (- more novel selection criterion) - analogies to existing solutions (InkSure \& CryptoGlyph)
  - alternative approach: colored dots form a discrete pattern, no grating necessary
- high res images could be used as camera APIs improve
- YCbCr suffers from clipping (https://en.wikipedia.org/wiki/YCbCr)
- binary thresholding (naive but simple and fits the context)
- color calibration (requires use of RAW data)
  - CCD > CMOS koska parempi herkkyys
- device detection (automatic configuration, database per phone model)
- iOS support (ios with c++? On iOS this extra bridge is unnecessary as C++ code can directly invoke Objective-C APIs.)
- more efficient matching if done server-side (leverage histogram data directly?)
  - easy to switch over because the offline-first approach taken
- change management / how to handle updates \& different devices?
- use histogram method. Fingerprint method useful for querying/filtering?
- more sophisticated peak finding / matching algorithms
- use of diffraction grating and spectral analysis
- get hold of the device or man-in-the-middle (metadata works as a layer of indirection)
using a predefined username and password (easily extendable to a real user auth)
- CouchDB uses a traditional cookie based authentication scheme, and as such, applications can be vulnerable to CSRF attacks. However, the application has no CSRF attacks vectors as it the DBMS and the application server is read-only
Traditional Cookie-Based Auth (could be extended to Modern Token-Based Auth such as JWT)

% \subsection{Taggant}
- phosphorescence vs. fluorescence (infeasible coz time)
- Afterglow brightness is also proportional to the intensity of UV contained in the excitation light.
- LumiNova pigments originally developed for use in watch dials
- Erilaisilla hiloilla, suodattimilla, raoilla ja peileilla on ehdottoman tarkea tehtavansa riittavan resoluution aikaansaamiseksi ja jotta saadaan valituksi tarkka viritysaallonpituus ja saadaan minimoiduksi sironta ja taustaluminesenssi. aikaerotteisen maarityksen etuna on se, etta viritysvalo ehtii sammua kokonaan ja sirontailmioiden aiheuttaman taustan vaikutus poistuu. Pitka elinika mahdollistaa myos edullisen ja varsin yksinkertaisen laitteiston kayton.122,124
- idempotence of the luminphores (photobleach)


\end{document}